+ pip install -r requirements.txt
Requirement already satisfied: colossalai>=0.1.12 in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.3.6)
Requirement already satisfied: torch>=1.8.1 in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.2.2)
Requirement already satisfied: numpy>=1.24.1 in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.26.4)
Requirement already satisfied: tqdm>=4.61.2 in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.66.2)
Requirement already satisfied: transformers>=4.20.0 in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (4.39.3)
Requirement already satisfied: datasets in ./.conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.18.0)
Requirement already satisfied: psutil in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.9.8)
Requirement already satisfied: packaging in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (24.0)
Requirement already satisfied: pre-commit in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.7.0)
Requirement already satisfied: rich in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (13.7.1)
Requirement already satisfied: click in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (8.1.7)
Requirement already satisfied: fabric in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.2.2)
Requirement already satisfied: contexttimer in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.3)
Requirement already satisfied: ninja in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (1.11.1.1)
Requirement already satisfied: safetensors in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.4.2)
Requirement already satisfied: einops in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.7.0)
Requirement already satisfied: pydantic in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.6.4)
Requirement already satisfied: ray in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.10.0)
Requirement already satisfied: sentencepiece in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.2.0)
Requirement already satisfied: google in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.0.0)
Requirement already satisfied: protobuf in ./.conda/lib/python3.9/site-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.26.1)
Requirement already satisfied: filelock in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.1)
Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)
Requirement already satisfied: networkx in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1)
Requirement already satisfied: jinja2 in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)
Requirement already satisfied: fsspec in ./.conda/lib/python3.9/site-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2024.2.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.conda/lib/python3.9/site-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.22.2)
Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.9/site-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.9/site-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2023.12.25)
Requirement already satisfied: requests in ./.conda/lib/python3.9/site-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.conda/lib/python3.9/site-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.15.2)
Requirement already satisfied: pyarrow>=12.0.0 in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (15.0.2)
Requirement already satisfied: pyarrow-hotfix in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (0.6)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (0.3.8)
Requirement already satisfied: pandas in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (2.2.1)
Requirement already satisfied: xxhash in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (3.4.1)
Requirement already satisfied: multiprocess in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (0.70.16)
Requirement already satisfied: aiohttp in ./.conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 6)) (3.9.3)
Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.9.4)
Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (4.0.3)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.9/site-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.9/site-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.9/site-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.9/site-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2024.2.2)
Requirement already satisfied: invoke>=2.0 in ./.conda/lib/python3.9/site-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.2.0)
Requirement already satisfied: paramiko>=2.4 in ./.conda/lib/python3.9/site-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)
Requirement already satisfied: decorator>=5 in ./.conda/lib/python3.9/site-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (5.1.1)
Requirement already satisfied: deprecated>=1.2 in ./.conda/lib/python3.9/site-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.2.14)
Requirement already satisfied: beautifulsoup4 in ./.conda/lib/python3.9/site-packages (from google->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.12.3)
Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.9/site-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 2)) (2.1.3)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 6)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 6)) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 6)) (2024.1)
Requirement already satisfied: cfgv>=2.0.0 in ./.conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)
Requirement already satisfied: identify>=1.0.0 in ./.conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5.35)
Requirement already satisfied: nodeenv>=0.11.1 in ./.conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.8.0)
Requirement already satisfied: virtualenv>=20.10.0 in ./.conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (20.25.1)
Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.9/site-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.6.0)
Requirement already satisfied: pydantic-core==2.16.3 in ./.conda/lib/python3.9/site-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.3)
Requirement already satisfied: jsonschema in ./.conda/lib/python3.9/site-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.21.1)
Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in ./.conda/lib/python3.9/site-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.0.8)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/lib/python3.9/site-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.9/site-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.17.2)
Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.9/site-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 2)) (1.3.0)
Requirement already satisfied: wrapt<2,>=1.10 in ./.conda/lib/python3.9/site-packages (from deprecated>=1.2->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.16.0)
Requirement already satisfied: mdurl~=0.1 in ./.conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.2)
Requirement already satisfied: setuptools in ./.conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (68.2.2)
Requirement already satisfied: bcrypt>=3.2 in ./.conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.1.2)
Requirement already satisfied: cryptography>=3.3 in ./.conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (42.0.5)
Requirement already satisfied: pynacl>=1.5 in ./.conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.5.0)
Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 6)) (1.16.0)
Requirement already satisfied: distlib<1,>=0.3.7 in ./.conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.8)
Requirement already satisfied: platformdirs<5,>=3.9.1 in ./.conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.2.0)
Requirement already satisfied: soupsieve>1.2 in ./.conda/lib/python3.9/site-packages (from beautifulsoup4->google->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.conda/lib/python3.9/site-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (2023.12.1)
Requirement already satisfied: referencing>=0.28.4 in ./.conda/lib/python3.9/site-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.34.0)
Requirement already satisfied: rpds-py>=0.7.1 in ./.conda/lib/python3.9/site-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.18.0)
Requirement already satisfied: cffi>=1.12 in ./.conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.16.0)
Requirement already satisfied: pycparser in ./.conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.22)
+ export BS=8
+ export MEMCAP=0
+ export GPUNUM=4
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin torch_ddp --batch_size 8
[2024-04-07 17:22:42,429] torch.distributed.run: [WARNING] 
[2024-04-07 17:22:42,429] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:22:42,429] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:22:42,429] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:22:46] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:22:49] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             torch_ddp                                          
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 37.96810460090637 seconds
[extension] Time taken to compile cpu_adam_x86 op: 37.80089807510376 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 51.72720289230347 seconds
[extension] Time taken to compile cpu_adam_x86 op: 89.62065601348877 seconds
[extension] Time taken to compile cpu_adam_x86 op: 89.537358045578 seconds
[extension] Time taken to compile fused_optim_cuda op: 51.75780200958252 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.5991487503051758 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.6053357124328613 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:24:22] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:02<00:42,  2.26s/it]
Training Step:  10%|█         | 2/20 [00:02<00:18,  1.03s/it]
Training Step:  15%|█▌        | 3/20 [00:02<00:10,  1.57it/s]
Training Step:  20%|██        | 4/20 [00:02<00:07,  2.23it/s]
Training Step:  25%|██▌       | 5/20 [00:02<00:05,  2.90it/s]
Training Step:  30%|███       | 6/20 [00:03<00:03,  3.55it/s]
Training Step:  35%|███▌      | 7/20 [00:03<00:03,  4.14it/s]
Training Step:  40%|████      | 8/20 [00:03<00:02,  4.65it/s]
Training Step:  45%|████▌     | 9/20 [00:03<00:02,  5.06it/s]
Training Step:  50%|█████     | 10/20 [00:03<00:01,  5.38it/s]
Training Step:  55%|█████▌    | 11/20 [00:03<00:01,  5.62it/s]
Training Step:  60%|██████    | 12/20 [00:04<00:01,  5.81it/s]
Training Step:  65%|██████▌   | 13/20 [00:04<00:01,  5.93it/s]
Training Step:  70%|███████   | 14/20 [00:04<00:00,  6.02it/s]
Training Step:  75%|███████▌  | 15/20 [00:04<00:00,  6.09it/s]
Training Step:  80%|████████  | 16/20 [00:04<00:00,  6.14it/s]
Training Step:  85%|████████▌ | 17/20 [00:04<00:00,  6.19it/s]
Training Step:  90%|█████████ | 18/20 [00:04<00:00,  6.23it/s]
Training Step:  95%|█████████▌| 19/20 [00:05<00:00,  6.24it/s]
Training Step: 100%|██████████| 20/20 [00:05<00:00,  6.26it/s]
Training Step: 100%|██████████| 20/20 [00:05<00:00,  3.77it/s]
[04/07/24 17:24:27] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 8, plugin: torch_ddp,          
                             throughput: 120.5087, maximum memory usage per gpu:
                             1.75 GB.                                           

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin torch_ddp_fp16 --batch_size 8
[2024-04-07 17:24:37,057] torch.distributed.run: [WARNING] 
[2024-04-07 17:24:37,057] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:24:37,057] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:24:37,057] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:24:41] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:24:43] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             torch_ddp_fp16                                     
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.2001969814300537 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.5360424518585205 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.6267893314361572 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.23274827003479004 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.8523046970367432 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.9401969909667969 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.26577186584472656 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.3047327995300293 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:24:48] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]
Training Step:   5%|▌         | 1/20 [00:02<00:48,  2.56s/it]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:  10%|█         | 2/20 [00:02<00:21,  1.19s/it]
Training Step:  20%|██        | 4/20 [00:02<00:08,  2.00it/s]
Training Step:  30%|███       | 6/20 [00:03<00:04,  3.29it/s]
Training Step:  40%|████      | 8/20 [00:03<00:02,  4.64it/s]
Training Step:  50%|█████     | 10/20 [00:03<00:01,  5.95it/s]
Training Step:  60%|██████    | 12/20 [00:03<00:01,  7.14it/s]
Training Step:  70%|███████   | 14/20 [00:03<00:00,  8.21it/s]
Training Step:  80%|████████  | 16/20 [00:03<00:00,  9.08it/s]
Training Step:  90%|█████████ | 18/20 [00:04<00:00,  9.82it/s]
Training Step: 100%|██████████| 20/20 [00:04<00:00,  9.99it/s]
Training Step: 100%|██████████| 20/20 [00:04<00:00,  4.59it/s]
[04/07/24 17:24:52] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 8, plugin: torch_ddp_fp16,     
                             throughput: 146.9361, maximum memory usage per gpu:
                             1.75 GB.                                           

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin low_level_zero --batch_size 8
[2024-04-07 17:25:01,499] torch.distributed.run: [WARNING] 
[2024-04-07 17:25:01,499] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:25:01,499] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:25:01,499] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:25:05] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:25:08] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             low_level_zero                                     
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.255810022354126 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.23311543464660645 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.38158249855041504 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.605518102645874 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.6383724212646484 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.7382934093475342 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.27124738693237305 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.304518461227417 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
[04/07/24 17:25:12] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:02<00:56,  2.96s/it]
Training Step:  10%|█         | 2/20 [00:03<00:24,  1.34s/it]
Training Step:  15%|█▌        | 3/20 [00:03<00:14,  1.21it/s]
Training Step:  20%|██        | 4/20 [00:03<00:09,  1.71it/s]
Training Step:  25%|██▌       | 5/20 [00:03<00:06,  2.22it/s]
Training Step:  30%|███       | 6/20 [00:04<00:05,  2.71it/s]
Training Step:  35%|███▌      | 7/20 [00:04<00:04,  2.96it/s]
Training Step:  40%|████      | 8/20 [00:04<00:03,  3.38it/s]
Training Step:  45%|████▌     | 9/20 [00:04<00:02,  3.75it/s]
Training Step:  50%|█████     | 10/20 [00:04<00:02,  3.94it/s]
Training Step:  55%|█████▌    | 11/20 [00:05<00:02,  4.21it/s]
Training Step:  60%|██████    | 12/20 [00:05<00:01,  4.40it/s]
Training Step:  65%|██████▌   | 13/20 [00:05<00:01,  4.56it/s]
Training Step:  70%|███████   | 14/20 [00:05<00:01,  4.64it/s]
Training Step:  75%|███████▌  | 15/20 [00:05<00:01,  4.70it/s]
Training Step:  80%|████████  | 16/20 [00:06<00:00,  4.73it/s]
Training Step:  85%|████████▌ | 17/20 [00:06<00:00,  4.75it/s]
Training Step:  90%|█████████ | 18/20 [00:06<00:00,  4.75it/s]
Training Step:  95%|█████████▌| 19/20 [00:06<00:00,  4.45it/s]
Training Step: 100%|██████████| 20/20 [00:07<00:00,  4.53it/s]
Training Step: 100%|██████████| 20/20 [00:07<00:00,  2.84it/s]
[04/07/24 17:25:19] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 8, plugin: low_level_zero,     
                             throughput: 90.8185, maximum memory usage per gpu: 
                             696.72 MB.                                         

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin gemini --batch_size 8
[2024-04-07 17:25:31,030] torch.distributed.run: [WARNING] 
[2024-04-07 17:25:31,030] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:25:31,030] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:25:31,030] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:25:35] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:25:37] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             gemini                                             
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.22114992141723633 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.22017812728881836 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.47033238410949707 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.6509702205657959 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.5043957233428955 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.6190550327301025 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.27312135696411133 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.30460619926452637 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
[04/07/24 17:25:42] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]
Training Step:   5%|▌         | 1/20 [00:02<00:49,  2.60s/it]
Training Step:  10%|█         | 2/20 [00:02<00:21,  1.22s/it]
Training Step:  15%|█▌        | 3/20 [00:03<00:13,  1.31it/s]
Training Step:  20%|██        | 4/20 [00:03<00:08,  1.79it/s]
Training Step:  25%|██▌       | 5/20 [00:03<00:06,  2.29it/s]
Training Step:  30%|███       | 6/20 [00:03<00:05,  2.79it/s]
Training Step:  35%|███▌      | 7/20 [00:03<00:04,  3.23it/s]
Training Step:  40%|████      | 8/20 [00:04<00:03,  3.60it/s]
Training Step:  45%|████▌     | 9/20 [00:04<00:02,  3.92it/s]
Training Step:  50%|█████     | 10/20 [00:04<00:02,  4.15it/s]
Training Step:  55%|█████▌    | 11/20 [00:04<00:02,  4.17it/s]
Training Step:  60%|██████    | 12/20 [00:05<00:01,  4.23it/s]
Training Step:  65%|██████▌   | 13/20 [00:05<00:01,  4.35it/s]
Training Step:  70%|███████   | 14/20 [00:05<00:01,  4.45it/s]
Training Step:  75%|███████▌  | 15/20 [00:05<00:01,  3.85it/s]
Training Step:  80%|████████  | 16/20 [00:06<00:01,  3.91it/s]
Training Step:  85%|████████▌ | 17/20 [00:06<00:00,  4.09it/s]
Training Step:  90%|█████████ | 18/20 [00:06<00:00,  4.28it/s]
Training Step:  95%|█████████▌| 19/20 [00:06<00:00,  4.42it/s]
Training Step: 100%|██████████| 20/20 [00:06<00:00,  4.51it/s]
Training Step: 100%|██████████| 20/20 [00:06<00:00,  2.89it/s]
[04/07/24 17:25:49] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 8, plugin: gemini, throughput: 
                             92.5829, maximum memory usage per gpu: 331.88 MB.  

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin hybrid_parallel --batch_size 8
[2024-04-07 17:26:00,573] torch.distributed.run: [WARNING] 
[2024-04-07 17:26:00,573] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:26:00,573] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:26:00,573] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:26:04] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:26:06] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             hybrid_parallel                                    
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.29753541946411133 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.2563631534576416 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.20515036582946777 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.43820762634277344 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.4341709613800049 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.3027501106262207 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.30903053283691406 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.3094148635864258 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:26:10] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:02<00:54,  2.86s/it]
Training Step:  10%|█         | 2/20 [00:03<00:24,  1.37s/it]
Training Step:  15%|█▌        | 3/20 [00:03<00:14,  1.14it/s]
Training Step:  20%|██        | 4/20 [00:03<00:10,  1.50it/s]
Training Step:  25%|██▌       | 5/20 [00:04<00:07,  1.88it/s]
Training Step:  30%|███       | 6/20 [00:04<00:06,  2.21it/s]
Training Step:  35%|███▌      | 7/20 [00:04<00:05,  2.49it/s]
Training Step:  40%|████      | 8/20 [00:05<00:04,  2.70it/s]
Training Step:  45%|████▌     | 9/20 [00:05<00:03,  2.87it/s]
Training Step:  50%|█████     | 10/20 [00:05<00:03,  2.96it/s]
Training Step:  55%|█████▌    | 11/20 [00:05<00:02,  3.03it/s]
Training Step:  60%|██████    | 12/20 [00:06<00:02,  3.04it/s]
Training Step:  65%|██████▌   | 13/20 [00:06<00:02,  3.02it/s]
Training Step:  70%|███████   | 14/20 [00:06<00:01,  3.09it/s]
Training Step:  75%|███████▌  | 15/20 [00:07<00:01,  3.11it/s]
Training Step:  80%|████████  | 16/20 [00:07<00:01,  3.06it/s]
Training Step:  85%|████████▌ | 17/20 [00:07<00:00,  3.12it/s]
Training Step:  90%|█████████ | 18/20 [00:08<00:00,  3.15it/s]
Training Step:  95%|█████████▌| 19/20 [00:08<00:00,  3.16it/s]
Training Step: 100%|██████████| 20/20 [00:08<00:00,  3.18it/s]
Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.27it/s]
[04/07/24 17:26:19] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 8, plugin: hybrid_parallel,    
                             throughput: 72.6727, maximum memory usage per gpu: 
                             417.03 MB.                                         

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin torch_ddp --batch_size 32
[2024-04-07 17:26:29,711] torch.distributed.run: [WARNING] 
[2024-04-07 17:26:29,711] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:26:29,711] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:26:29,711] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:26:33] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:26:35] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             torch_ddp                                          
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.23541951179504395 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.32173919677734375 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.23732209205627441 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.24080801010131836 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.3358266353607178 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.2289595603942871 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.20997929573059082 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.30440735816955566 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:26:39] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:02<00:41,  2.19s/it]
Training Step:  10%|█         | 2/20 [00:02<00:22,  1.24s/it]
Training Step:  15%|█▌        | 3/20 [00:03<00:16,  1.06it/s]
Training Step:  20%|██        | 4/20 [00:03<00:12,  1.24it/s]
Training Step:  25%|██▌       | 5/20 [00:04<00:10,  1.37it/s]
Training Step:  30%|███       | 6/20 [00:05<00:09,  1.47it/s]
Training Step:  35%|███▌      | 7/20 [00:05<00:08,  1.53it/s]
Training Step:  40%|████      | 8/20 [00:06<00:07,  1.58it/s]
Training Step:  45%|████▌     | 9/20 [00:06<00:06,  1.62it/s]
Training Step:  50%|█████     | 10/20 [00:07<00:06,  1.64it/s]
Training Step:  55%|█████▌    | 11/20 [00:08<00:05,  1.66it/s]
Training Step:  60%|██████    | 12/20 [00:08<00:04,  1.67it/s]
Training Step:  65%|██████▌   | 13/20 [00:09<00:04,  1.68it/s]
Training Step:  70%|███████   | 14/20 [00:09<00:03,  1.68it/s]
Training Step:  75%|███████▌  | 15/20 [00:10<00:02,  1.69it/s]
Training Step:  80%|████████  | 16/20 [00:11<00:02,  1.70it/s]
Training Step:  85%|████████▌ | 17/20 [00:11<00:01,  1.70it/s]
Training Step:  90%|█████████ | 18/20 [00:12<00:01,  1.70it/s]
Training Step:  95%|█████████▌| 19/20 [00:12<00:00,  1.70it/s]
Training Step: 100%|██████████| 20/20 [00:13<00:00,  1.69it/s]
Training Step: 100%|██████████| 20/20 [00:13<00:00,  1.50it/s]
[04/07/24 17:26:52] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 32, plugin: torch_ddp,         
                             throughput: 191.3859, maximum memory usage per gpu:
                             2.13 GB.                                           

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin torch_ddp_fp16 --batch_size 32
[2024-04-07 17:27:03,891] torch.distributed.run: [WARNING] 
[2024-04-07 17:27:03,891] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:27:03,891] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:27:03,891] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:27:07] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:27:09] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             torch_ddp_fp16                                     
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.25718116760253906 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.14106225967407227 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.13637685775756836 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.5148813724517822 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.7383005619049072 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.19340991973876953 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.8051981925964355 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.8049600124359131 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:27:14] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:02<00:39,  2.08s/it]
Training Step:  10%|█         | 2/20 [00:02<00:17,  1.03it/s]
Training Step:  15%|█▌        | 3/20 [00:02<00:10,  1.64it/s]
Training Step:  20%|██        | 4/20 [00:02<00:07,  2.27it/s]
Training Step:  25%|██▌       | 5/20 [00:02<00:05,  2.88it/s]
Training Step:  30%|███       | 6/20 [00:02<00:04,  3.44it/s]
Training Step:  35%|███▌      | 7/20 [00:03<00:03,  3.93it/s]
Training Step:  40%|████      | 8/20 [00:03<00:02,  4.33it/s]
Training Step:  45%|████▌     | 9/20 [00:03<00:02,  4.64it/s]
Training Step:  50%|█████     | 10/20 [00:03<00:02,  4.89it/s]
Training Step:  55%|█████▌    | 11/20 [00:03<00:01,  5.07it/s]
Training Step:  60%|██████    | 12/20 [00:04<00:01,  5.22it/s]
Training Step:  65%|██████▌   | 13/20 [00:04<00:01,  5.31it/s]
Training Step:  70%|███████   | 14/20 [00:04<00:01,  5.39it/s]
Training Step:  75%|███████▌  | 15/20 [00:04<00:00,  5.44it/s]
Training Step:  80%|████████  | 16/20 [00:04<00:00,  5.47it/s]
Training Step:  85%|████████▌ | 17/20 [00:04<00:00,  5.50it/s]
Training Step:  90%|█████████ | 18/20 [00:05<00:00,  5.50it/s]
Training Step:  95%|█████████▌| 19/20 [00:05<00:00,  5.50it/s]
Training Step: 100%|██████████| 20/20 [00:05<00:00,  5.51it/s]
Training Step: 100%|██████████| 20/20 [00:05<00:00,  3.62it/s]
[04/07/24 17:27:19] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 32, plugin: torch_ddp_fp16,    
                             throughput: 463.2753, maximum memory usage per gpu:
                             2.05 GB.                                           

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin low_level_zero --batch_size 32
[2024-04-07 17:27:28,081] torch.distributed.run: [WARNING] 
[2024-04-07 17:27:28,081] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:27:28,081] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:27:28,081] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:27:31] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:27:33] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             low_level_zero                                     
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.17099547386169434 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.2073497772216797 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.13569140434265137 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.13233375549316406 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.23826146125793457 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.43384623527526855 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.30410051345825195 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.27214956283569336 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
[04/07/24 17:27:36] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:03<01:01,  3.22s/it]
Training Step:  10%|█         | 2/20 [00:03<00:26,  1.49s/it]
Training Step:  15%|█▌        | 3/20 [00:03<00:15,  1.07it/s]
Training Step:  20%|██        | 4/20 [00:04<00:10,  1.48it/s]
Training Step:  25%|██▌       | 5/20 [00:04<00:07,  1.88it/s]
Training Step:  30%|███       | 6/20 [00:04<00:06,  2.24it/s]
Training Step:  35%|███▌      | 7/20 [00:04<00:05,  2.55it/s]
Training Step:  40%|████      | 8/20 [00:05<00:04,  2.81it/s]
Training Step:  45%|████▌     | 9/20 [00:05<00:03,  3.01it/s]
Training Step:  50%|█████     | 10/20 [00:05<00:03,  3.17it/s]
Training Step:  55%|█████▌    | 11/20 [00:06<00:02,  3.29it/s]
Training Step:  60%|██████    | 12/20 [00:06<00:02,  3.36it/s]
Training Step:  65%|██████▌   | 13/20 [00:06<00:02,  3.43it/s]
Training Step:  70%|███████   | 14/20 [00:06<00:01,  3.47it/s]
Training Step:  75%|███████▌  | 15/20 [00:07<00:01,  3.51it/s]
Training Step:  80%|████████  | 16/20 [00:07<00:01,  3.53it/s]
Training Step:  85%|████████▌ | 17/20 [00:07<00:00,  3.56it/s]
Training Step:  90%|█████████ | 18/20 [00:07<00:00,  3.57it/s]
Training Step:  95%|█████████▌| 19/20 [00:08<00:00,  3.59it/s]
Training Step: 100%|██████████| 20/20 [00:08<00:00,  3.58it/s]
Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.35it/s]
[04/07/24 17:27:44] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 32, plugin: low_level_zero,    
                             throughput: 300.5291, maximum memory usage per gpu:
                             890.97 MB.                                         

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin gemini --batch_size 32
[2024-04-07 17:27:52,385] torch.distributed.run: [WARNING] 
[2024-04-07 17:27:52,385] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:27:52,385] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:27:52,385] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:27:56] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:27:58] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             gemini                                             
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.1933426856994629 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.3179910182952881 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.5203335285186768 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.3371706008911133 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.256702184677124 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.304699182510376 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.5373272895812988 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.24159646034240723 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return tensor.storage().size() == 0
[04/07/24 17:28:02] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]
Training Step:   5%|▌         | 1/20 [00:02<00:40,  2.15s/it]
Training Step:  10%|█         | 2/20 [00:02<00:18,  1.02s/it]
Training Step:  15%|█▌        | 3/20 [00:02<00:11,  1.52it/s]
Training Step:  20%|██        | 4/20 [00:02<00:07,  2.05it/s]
Training Step:  25%|██▌       | 5/20 [00:03<00:05,  2.55it/s]
Training Step:  30%|███       | 6/20 [00:03<00:04,  3.00it/s]
Training Step:  35%|███▌      | 7/20 [00:03<00:03,  3.36it/s]
Training Step:  40%|████      | 8/20 [00:03<00:03,  3.67it/s]
Training Step:  45%|████▌     | 9/20 [00:03<00:02,  3.85it/s]
Training Step:  50%|█████     | 10/20 [00:04<00:02,  4.02it/s]
Training Step:  55%|█████▌    | 11/20 [00:04<00:02,  4.15it/s]
Training Step:  60%|██████    | 12/20 [00:04<00:02,  3.58it/s]
Training Step:  65%|██████▌   | 13/20 [00:05<00:01,  3.69it/s]
Training Step:  70%|███████   | 14/20 [00:05<00:01,  3.65it/s]
Training Step:  75%|███████▌  | 15/20 [00:05<00:01,  3.91it/s]
Training Step:  80%|████████  | 16/20 [00:05<00:00,  4.06it/s]
Training Step:  85%|████████▌ | 17/20 [00:05<00:00,  3.99it/s]
Training Step:  90%|█████████ | 18/20 [00:06<00:00,  4.20it/s]
Training Step:  95%|█████████▌| 19/20 [00:06<00:00,  4.30it/s]
Training Step: 100%|██████████| 20/20 [00:06<00:00,  4.40it/s]
Training Step: 100%|██████████| 20/20 [00:06<00:00,  3.01it/s][04/07/24 17:28:09] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         

                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 32, plugin: gemini, throughput:
                             385.2396, maximum memory usage per gpu: 523.21 MB. 

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
+ MODEL_PATH=google/vit-base-patch16-224
+ colossalai run --nproc_per_node 4 --master_port 29505 vit_benchmark.py --model_name_or_path google/vit-base-patch16-224 --mem_cap 0 --plugin hybrid_parallel --batch_size 32
[2024-04-07 17:28:16,696] torch.distributed.run: [WARNING] 
[2024-04-07 17:28:16,696] torch.distributed.run: [WARNING] *****************************************
[2024-04-07 17:28:16,696] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-07 17:28:16,696] torch.distributed.run: [WARNING] *****************************************
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel
  warnings.warn("Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel")
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.
  warnings.warn("`config` is deprecated and will be removed soon.")
[04/07/24 17:28:20] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /.conda/lib/python3.9/site-packages/colossalai/init
                             ialize.py:67 launch                                
                    INFO     colossalai - colossalai - INFO: Distributed        
                             environment is initialized, world size: 4          
[04/07/24 17:28:22] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:68 main                          
                    INFO     colossalai - colossalai - INFO: Finish loading     
                             model from google/vit-base-patch16-224             
                    INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:94 main                          
                    INFO     colossalai - colossalai - INFO: Set plugin as      
                             hybrid_parallel                                    
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
[extension] Time taken to compile cpu_adam_x86 op: 0.19873785972595215 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (11.0) does not match with the version (11.8) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions
  warnings.warn(
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.5242993831634521 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.738797664642334 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.5347747802734375 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.18734502792358398 seconds
[extension] Time taken to compile fused_optim_cuda op: 0.2040863037109375 seconds
[extension] Time taken to compile cpu_adam_x86 op: 0.9296793937683105 seconds
[extension] Compiling the JIT fused_optim_cuda kernel during runtime now
[extension] Time taken to compile fused_optim_cuda op: 0.22480988502502441 seconds
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403392949/work/torch/csrc/tensor/python_tensor.cpp:83.)
  self._dummy_overflow_buf = torch.cuda.IntTensor([0])
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx5-rdmav25.so': /usr/lib/libibverbs/libmlx5-rdmav25.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav25.so': /usr/lib/libibverbs/libmlx4-rdmav25.so: cannot open shared object file: No such file or directory
[04/07/24 17:28:26] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:108 main                         
                    INFO     colossalai - colossalai - INFO: Start testing      

Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()
/storage_fast/user/ViT-Fine-Tuning-with-ColossalAI/.conda/lib/python3.9/site-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  numel += p.storage().size()

Training Step:   5%|▌         | 1/20 [00:03<01:09,  3.63s/it]
Training Step:  10%|█         | 2/20 [00:04<00:39,  2.17s/it]
Training Step:  15%|█▌        | 3/20 [00:05<00:29,  1.71s/it]
Training Step:  20%|██        | 4/20 [00:07<00:23,  1.49s/it]
Training Step:  25%|██▌       | 5/20 [00:08<00:20,  1.37s/it]
Training Step:  30%|███       | 6/20 [00:09<00:18,  1.29s/it]
Training Step:  35%|███▌      | 7/20 [00:10<00:16,  1.24s/it]
Training Step:  40%|████      | 8/20 [00:11<00:14,  1.22s/it]
Training Step:  45%|████▌     | 9/20 [00:12<00:13,  1.20s/it]
Training Step:  50%|█████     | 10/20 [00:14<00:11,  1.19s/it]
Training Step:  55%|█████▌    | 11/20 [00:15<00:10,  1.18s/it]
Training Step:  60%|██████    | 12/20 [00:16<00:09,  1.18s/it]
Training Step:  65%|██████▌   | 13/20 [00:17<00:08,  1.18s/it]
Training Step:  70%|███████   | 14/20 [00:18<00:07,  1.19s/it]
Training Step:  75%|███████▌  | 15/20 [00:19<00:05,  1.19s/it]
Training Step:  80%|████████  | 16/20 [00:21<00:04,  1.18s/it]
Training Step:  85%|████████▌ | 17/20 [00:22<00:03,  1.17s/it]
Training Step:  90%|█████████ | 18/20 [00:23<00:02,  1.17s/it]
Training Step:  95%|█████████▌| 19/20 [00:24<00:01,  1.16s/it]
Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.16s/it]
Training Step: 100%|██████████| 20/20 [00:25<00:00,  1.29s/it]
[04/07/24 17:28:52] INFO     colossalai - colossalai - INFO:                    
                             /storage_fast/user/ViT-Fine-Tuning-with-ColossalAI
                             /vit_benchmark.py:140 main                         
                    INFO     colossalai - colossalai - INFO: Testing finished,  
                             batch size per gpu: 32, plugin: hybrid_parallel,   
                             throughput: 99.5983, maximum memory usage per gpu: 
                             431.50 MB.                                         

====== Training on All Nodes =====
127.0.0.1: success

====== Stopping All Nodes =====
127.0.0.1: finish
